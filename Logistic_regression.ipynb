{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+FQ68Uv3jkR2YtBEznY6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Metamorphozis/model_ml/blob/main/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDW0uVS9Pp-O",
        "outputId": "1ffe7e79-63cc-47a7-8e9e-a8540ef70925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16637872008745783\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "class MyLogReg:\n",
        "    def __init__(self, n_iter=10, learning_rate=0.1, weights=None, l1_reg=0, l2_reg=0, elastic_net_alpha=0, elastic_net_l1_ratio=0.5, sgd_sample=None, random_state=42, metric=None):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = weights if weights is not None else np.zeros(16)\n",
        "        self.l1_reg = l1_reg\n",
        "        self.l2_reg = l2_reg\n",
        "        self.elastic_net_alpha = elastic_net_alpha\n",
        "        self.elastic_net_l1_ratio = elastic_net_l1_ratio\n",
        "        self.sgd_sample = sgd_sample\n",
        "        self.random_state = random_state\n",
        "        self.metric = metric\n",
        "        self.best_score = None\n",
        "\n",
        "    def fit(self, X, y, verbose=False):\n",
        "        random.seed(self.random_state)\n",
        "        X = pd.DataFrame(X)\n",
        "        X = pd.concat([pd.DataFrame(np.ones((X.shape[0], 1))), X], axis=1)\n",
        "        num_features = X.shape[1]\n",
        "        self.weights = np.zeros(num_features)\n",
        "\n",
        "        if self.sgd_sample is not None:\n",
        "            sample_size = int(self.sgd_sample * X.shape[0]) if isinstance(self.sgd_sample, float) else self.sgd_sample\n",
        "            # Ограничиваем sample_size размером X.shape[0]\n",
        "            sample_size = min(sample_size, X.shape[0])\n",
        "            sample_indices = random.choices(range(X.shape[0]), k=sample_size)\n",
        "            X_sample = X.iloc[sample_indices]\n",
        "            y_sample = np.take(y, sample_indices)\n",
        "        else:\n",
        "            X_sample = X\n",
        "            y_sample = y\n",
        "\n",
        "        y_pred = 1 / (1 + np.exp(-np.dot(X_sample, self.weights)))\n",
        "        log_loss_start = -(y_sample * np.log(y_pred + 1e-15) + (1 - y_sample) * np.log(1 - y_pred + 1e-15)).mean()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"start | loss: {log_loss_start:.2f}\", end=\" | \")\n",
        "\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            if self.sgd_sample is not None:\n",
        "                sample_size = int(self.sgd_sample * X.shape[0]) if isinstance(self.sgd_sample, float) else self.sgd_sample\n",
        "                # Ограничиваем sample_size размером X.shape[0]\n",
        "                sample_size = min(sample_size, X.shape[0])\n",
        "                sample_indices = random.choices(range(X.shape[0]), k=sample_size)\n",
        "                X_sample = X.iloc[sample_indices]\n",
        "                y_sample = np.take(y, sample_indices)\n",
        "            else:\n",
        "                X_sample = X\n",
        "                y_sample = y\n",
        "\n",
        "            y_pred_sample = 1 / (1 + np.exp(-np.dot(X_sample, self.weights)))\n",
        "            log_loss_sample = -(y_sample * np.log(y_pred_sample + 1e-15) + (1 - y_sample) * np.log(1 - y_pred_sample + 1e-15)).mean()\n",
        "\n",
        "            grad = np.dot(X_sample.T, (y_pred_sample - y_sample)) / X_sample.shape[0]\n",
        "\n",
        "            # Обновляем веса с учетом регуляризации\n",
        "            if self.l1_reg > 0:\n",
        "                reg_term = self.l1_reg * np.sign(self.weights)\n",
        "            elif self.l2_reg > 0:\n",
        "                reg_term = self.l2_reg * self.weights\n",
        "            elif self.elastic_net_alpha > 0:\n",
        "                l1_reg = self.elastic_net_alpha * self.elastic_net_l1_ratio\n",
        "                l2_reg = self.elastic_net_alpha * (1 - self.elastic_net_l1_ratio)\n",
        "                reg_term = l1_reg * np.sign(self.weights) + l2_reg * self.weights\n",
        "            else:\n",
        "                reg_term = 0\n",
        "\n",
        "            self.weights -= self.learning_rate * (grad + reg_term)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{i} | loss: {log_loss_sample:.2f} | learning_rate: {self.learning_rate:.2f}\", end=\" | \")\n",
        "                if self.metric:\n",
        "                    y_pred_full = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "                    score = self._calculate_metric(y, (y_pred_full > 0.5).astype(int))\n",
        "                    print(f\"{self.metric}: {score:.2f}\")\n",
        "                else:\n",
        "                    print(\"\")\n",
        "\n",
        "        # Запоминаем best_score после обучения\n",
        "        if self.metric:\n",
        "            y_pred_full = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "            self.best_score = self._calculate_metric(y, (y_pred_full > 0.5).astype(int))\n",
        "\n",
        "    def get_coef(self):\n",
        "        return self.weights\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = pd.concat([pd.DataFrame(np.ones((X.shape[0], 1))), X], axis=1)\n",
        "        y_pred = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "        return y_pred\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = pd.concat([pd.DataFrame(np.ones((X.shape[0], 1))), X], axis=1)\n",
        "        y_pred = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "        y_pred_class = (y_pred > 0.5).astype(int)\n",
        "        return y_pred_class\n",
        "\n",
        "    def get_best_score(self):\n",
        "        if self.best_score is None:\n",
        "            return \"No score available\"\n",
        "        else:\n",
        "            return self.best_score\n",
        "\n",
        "    def _calculate_metric(self, y_true, y_pred):\n",
        "        if self.metric == 'accuracy':\n",
        "            score = accuracy_score(y_true, y_pred)\n",
        "        elif self.metric == 'precision':\n",
        "            score = precision_score(y_true, y_pred)\n",
        "        elif self.metric == 'recall':\n",
        "            score = recall_score(y_true, y_pred)\n",
        "        elif self.metric == 'f1':\n",
        "            score = f1_score(y_true, y_pred)\n",
        "        elif self.metric == 'roc_auc':\n",
        "            score = roc_auc_score(y_true, y_pred)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported metric\")\n",
        "        return score\n",
        "\n",
        "# Тестовые данные\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Генерация тестовых данных\n",
        "X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Модель с sgd_sample = 10\n",
        "log = MyLogReg(n_iter=50, learning_rate=0.1, sgd_sample=0.1)\n",
        "log.fit(X_train, y_train)\n",
        "print(np.mean(log.get_coef()))"
      ]
    }
  ]
}