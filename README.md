# model_ml

# Модели машинного обучения. Конспект + практика

## Линейные модели. Реализация модели линейной регрессии - Linear_regression

### Линейная функция

**Линейная регрессия (Linear regression)** - метод машинного обучения, в котором ищется отношение зависимой переменной от одной или нескольких независимых переменных (регрессоров) посредством линейной функции. \
Линейная функция записывается уравнением $y = b + k*x$ \
где: \
 y - зависимая (целевая) переменная \
 x - независимая переменная (регрессор) \
 k - коэффициент наклона (Тангенс угла наклона - определяет наклон линии относительно оси абсцисс (x)) \
 b - отрезок или свободный коэффициент (определяет смещение линии относительно оси абсцисс (x) и где линия пересекает ось ординат (y)) <br>

В машинном обучении уравнение имеет вид: \
$y = w_0 + w_1 * x_1$ \
$y = w_0 + w_1 * x1 + w_2 * x_2$ (в трехмерном пространстве) \
$y = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_m * x_m$ (в m-мерном пространстве) \
Тут коэффициенты k и b принято называть весами и обозначать их w, x - это фича, а y - целевая переменная. \
Количество весов напрямую зависит от количества фичей и задача линейной регресси подобрать такие веса w, которые дадут наилучший результат при предсказании.<br>

### Метод наименьших квадратов

Один из способов найти наилучшую зависимость - Метод наименьших квадратов (МНК, Least squares) - математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений выходных значений некоторой функции от исходных значений. \
Часто для реализации МНК используется функция MSE (от англ. Mean Squared Error – средняя квадратичная ошибка)

$$ MSE = \frac{1}{n} \sum\_{i=1}^{n} (y_i - \hat{y}\_i)^2 <br> $$

Код из файла step_1_linear_regression.py выведет:\
'''
start | loss: 3.00
100 | loss: 0.37
200 | loss: 0.18
300 | loss: 0.09
Коэффициенты модели: x1 1.162258
x2 1.324516
dtype: float64
'''

## Линейные модели. Реализация модели логистической регрессии регрессии - Logistic_regression
