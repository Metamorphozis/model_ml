# model_ml
# Модели машинного обучения. Конспект + практика #

## Линейные модели. Реализация модели линейной регрессии - Linear_regression ##

### Линейная функция ###
**Линейная регрессия (Linear regression)** - метод машинного обучения, в котором ищется отношение зависимой переменной от одной или нескольких независимых переменных (регрессоров) посредством линейной функции. \
Линейная функция записывается уравнением $y = b + k*x$ \
где: \
  y - зависимая (целевая) переменная \
  x - независимая переменная (регрессор) \
  k - коэффициент наклона (Тангенс угла наклона - определяет наклон линии относительно оси абсцисс (x)) \
  b - отрезок или свободный коэффициент (определяет смещение линии относительно оси абсцисс (x) и где линия пересекает ось ординат (y)) <br>

В машинном обучении уравнение имеет вид: \
$y = w_0 + w_1 * x_1$ \
$y = w_0 + w_1 * x1 + w_2 * x_2$ (в трехмерном пространстве) \
$y = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_m * x_m$ (в m-мерном пространстве) \
Тут коэффициенты k и b принято называть весами и обозначать их w, x - это фича, а y - целевая переменная. \
Количество весов напрямую зависит от количества фичей и задача линейной регресси подобрать такие веса w, которые дадут наилучший результат при предсказании.<br>

### Метод наименьших квадратов ###

Один из способов найти наилучшую зависимость - Метод наименьших квадратов (МНК, Least squares) - математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений выходных значений некоторой функции от исходных значений. \
Часто для реализации МНК используется функция MSE (от англ. Mean Squared Error – средняя квадратичная ошибка) 

$$ MSE =  \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 <br>  $$

## Линейные модели. Реализация модели логистической регрессии регрессии - Logistic_regression ##
